{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "BERT Representation Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "sample_text = 'Deploying a large-scale distributed ecosystem such as HBase/Hadoop in the cloud is complicated and error-prone. Multiple layers of largely independently evolving software are deployed across distributed nodes on third party infrastructures. In addition to software incompatibility and typical misconfiguration within each layer, many subtle and hard to diagnose errors happen due to misconfigurations across layers and nodes. These errors are difficult to diagnose because of scattered log management and lack of ecosystem-awareness in many diagnosis tools and processes. We report on some failure experiences in a real world deployment of HBase/Hadoop and propose some initial ideas for better trouble-shooting during deployment. We identify the following types of subtle errors and the corresponding challenges in trouble-shooting: 1) dealing with inconsistency among distributed logs, 2) distinguishing useful information from noisy logging, and 3) probabilistic determination of root causes.'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "%% script false --no-raise-error\n",
    "import re\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "def text_preprocess(text):\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.casefold()\n",
    "    text = remove_stopwords(text)\n",
    "\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "%% script false --no-raise-error\n",
    "processed_text = text_preprocess(sample_text)\n",
    "processed_text = '[CLS] ' + processed_text + ' [SEP]'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "deploy       21,296\n",
      "##ing         2,075\n",
      "large         2,312\n",
      "-             1,011\n",
      "scale         4,094\n",
      "distributed   5,500\n",
      "ecosystem    16,927\n",
      "h             1,044\n",
      "##base       15,058\n",
      "/             1,013\n",
      "had           2,018\n",
      "##oop        18,589\n",
      "cloud         6,112\n",
      "complicated   8,552\n",
      "error         7,561\n",
      "-             1,011\n",
      "prone        13,047\n",
      ".             1,012\n",
      "multiple      3,674\n",
      "layers        9,014\n",
      "largely       4,321\n",
      "independently  9,174\n",
      "evolving     20,607\n",
      "software      4,007\n",
      "deployed      7,333\n",
      "distributed   5,500\n",
      "nodes        14,164\n",
      "party         2,283\n",
      "infrastructure  6,502\n",
      "##s           2,015\n",
      ".             1,012\n",
      "addition      2,804\n",
      "software      4,007\n",
      "inc           4,297\n",
      "##omp        25,377\n",
      "##ati        10,450\n",
      "##bility      8,553\n",
      "typical       5,171\n",
      "mis          28,616\n",
      "##con         8,663\n",
      "##fi          8,873\n",
      "##gur        27,390\n",
      "##ation       3,370\n",
      "layer         6,741\n",
      ",             1,010\n",
      "subtle       11,259\n",
      "hard          2,524\n",
      "dia          22,939\n",
      "##gno        26,745\n",
      "##se          3,366\n",
      "errors       10,697\n",
      "happen        4,148\n",
      "mis          28,616\n",
      "##con         8,663\n",
      "##fi          8,873\n",
      "##gur        27,390\n",
      "##ations     10,708\n",
      "layers        9,014\n",
      "nodes        14,164\n",
      ".             1,012\n",
      "errors       10,697\n",
      "difficult     3,697\n",
      "dia          22,939\n",
      "##gno        26,745\n",
      "##se          3,366\n",
      "scattered     7,932\n",
      "log           8,833\n",
      "management    2,968\n",
      "lack          3,768\n",
      "ecosystem    16,927\n",
      "-             1,011\n",
      "awareness     7,073\n",
      "diagnosis    11,616\n",
      "tools         5,906\n",
      "processes     6,194\n",
      ".             1,012\n",
      "report        3,189\n",
      "failure       4,945\n",
      "experiences   6,322\n",
      "real          2,613\n",
      "world         2,088\n",
      "deployment   10,813\n",
      "h             1,044\n",
      "##base       15,058\n",
      "/             1,013\n",
      "had           2,018\n",
      "##oop        18,589\n",
      "propose      16,599\n",
      "initial       3,988\n",
      "ideas         4,784\n",
      "better        2,488\n",
      "trouble       4,390\n",
      "-             1,011\n",
      "shooting      5,008\n",
      "deployment   10,813\n",
      ".             1,012\n",
      "identify      6,709\n",
      "following     2,206\n",
      "types         4,127\n",
      "subtle       11,259\n",
      "errors       10,697\n",
      "corresponding  7,978\n",
      "challenges    7,860\n",
      "trouble       4,390\n",
      "-             1,011\n",
      "shooting      5,008\n",
      ":             1,024\n",
      ")             1,007\n",
      "dealing       7,149\n",
      "inc           4,297\n",
      "##ons         5,644\n",
      "##iste       27,870\n",
      "##ncy         9,407\n",
      "distributed   5,500\n",
      "logs         15,664\n",
      ",             1,010\n",
      ")             1,007\n",
      "distinguishing 20,852\n",
      "useful        6,179\n",
      "information   2,592\n",
      "noisy        20,810\n",
      "logging      15,899\n",
      ",             1,010\n",
      ")             1,007\n",
      "pro           4,013\n",
      "##ba          3,676\n",
      "##bilis      27,965\n",
      "##tic         4,588\n",
      "determination  9,128\n",
      "root          7,117\n",
      "causes        5,320\n",
      ".             1,012\n",
      "[SEP]           102\n"
     ]
    }
   ],
   "source": [
    "%% script false --no-raise-error\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "tokenized_text = tokenizer.tokenize(processed_text)\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))\n",
    "\n",
    "segments_id = [1] * len(tokenized_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": "BertModel(\n  (embeddings): BertEmbeddings(\n    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (token_type_embeddings): Embedding(2, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): BertEncoder(\n    (layer): ModuleList(\n      (0): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%% script false --no-raise-error\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True)\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "%% script false --no-raise-error\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensor = torch.tensor([segments_id])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "%% script false --no-raise-error\n",
    "with torch.no_grad():\n",
    "    output = model(tokens_tensor, segments_tensor)\n",
    "    # get hidden state from all layers\n",
    "    hidden_states = output[2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
      "Number of batches (sequences): 1\n",
      "Number of tokens: 134\n",
      "Number of hidden units (tensor size): 768\n"
     ]
    }
   ],
   "source": [
    "%% script false --no-raise-error\n",
    "print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches (sequences):\", len(hidden_states[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "\n",
    "print (\"Number of hidden units (tensor size):\", len(hidden_states[layer_i][batch_i][token_i]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Type of hidden_states:  <class 'tuple'>\n",
      "Tensor shape for each layer:  torch.Size([1, 132, 768])\n"
     ]
    }
   ],
   "source": [
    "%% script false --no-raise-error\n",
    "# `hidden_states` is a Python list.\n",
    "print('      Type of hidden_states: ', type(hidden_states))\n",
    "\n",
    "# Each layer in the list is a torch tensor.\n",
    "print('Tensor shape for each layer: ', hidden_states[0].size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([13, 1, 134, 768])"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%% script false --no-raise-error\n",
    "# Concatenate the tensors for all layers. We use `stack` here to\n",
    "# create a new dimension in the tensor.\n",
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "token_embeddings.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([13, 134, 768])"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%% script false --no-raise-error\n",
    "# Remove dimension 1, the \"batches\". (since this has only 1 batch)\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "token_embeddings.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([134, 13, 768])"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%% script false --no-raise-error\n",
    "# Swap dimensions 0 and 1.\n",
    "# from [layers, tokens, hidden units] to [tokens, layers, hidden units]\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_embeddings.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 134 x 768\n"
     ]
    }
   ],
   "source": [
    "%% script false --no-raise-error\n",
    "# Get token embeddings by summing the last 4 layers for each token\n",
    "token_vecs_sum = []\n",
    "\n",
    "# `token_embeddings` is a [190 x 13 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "data": {
      "text/plain": "768"
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%% script false --no-raise-error\n",
    "token_vecs_sum[0].shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([768])"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%% script false --no-raise-error\n",
    "index_of_token = [i for i, x in enumerate(tokenized_text) if x == 'distributed']\n",
    "token_vecs = [token_vecs_sum[i] for i in index_of_token]\n",
    "token_tensors = torch.stack(token_vecs)\n",
    "token_vecs_avg = torch.mean(token_tensors, dim=0)\n",
    "token_vecs_avg.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20884549617767334\n",
      "0.2738270163536072\n",
      "0.32876014709472656\n"
     ]
    }
   ],
   "source": [
    "%% script false --no-raise-error\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "print(cosine(token_vecs_sum[index_of_token[0]], token_vecs_sum[index_of_token[1]]))\n",
    "print(cosine(token_vecs_sum[index_of_token[2]], token_vecs_sum[index_of_token[1]]))\n",
    "print(cosine(token_vecs_sum[index_of_token[0]], token_vecs_sum[index_of_token[2]]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09470295906066895\n",
      "0.07165509462356567\n",
      "0.11758136749267578\n"
     ]
    }
   ],
   "source": [
    "%% script false --no-raise-error\n",
    "for i in index_of_token:\n",
    "    print(cosine(token_vecs_sum[i], token_vecs_avg))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mis', '##con', '##fi', '##gur', '##ation']\n",
      "['had', '##oop']\n",
      "['infrastructure', '##s']\n",
      "['deploy', '##ing']\n",
      "['cloud', 'computing']\n",
      "['face', 'recognition']\n",
      "['em', '##bed', '##ding', '##s']\n"
     ]
    }
   ],
   "source": [
    "%% script false --no-raise-error\n",
    "sample_term = ['misconfiguration', 'hadoop', 'infrastructures', 'deploying', 'cloud computing', 'face recognition', 'embeddings']\n",
    "for term in sample_term:\n",
    "    sample_token = tokenizer.tokenize(term)\n",
    "    print(sample_token)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "%% script false --no-raise-error\n",
    "sample_text_2 = 'Several investigations [11,16,19–21] have recently been undertaken into object recognition based on matching image intensity neighborhoods rather than geometric matching of features extracted from the images. These projects have used small subwindows or complete image regions and matching has been based on the similarity of extracted descriptors to previously stored descriptors. One characteristic common to these approaches is the representation of objects as a whole, rather than as a structured ensemble. This paper describes an extension to these approaches wherein a set of related features recognized at an earlier iteration also contribute to the complete object recognition. The paper describes an iconic, or image-based, matching approach that incorporates an element of geometric matching and shows that use of the subfeatures improves matching efficiency, position accuracy and completeness.'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "%% script false --no-raise-error\n",
    "text_list = [sample_text, sample_text_2]\n",
    "kw_list = ['object recognition', 'geometric matching', 'distributed ecosystem', 'distributed nodes']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "%% script false --no-raise-error\n",
    "def encode_text(text, model, tokenizer):\n",
    "    text = '[CLS] ' + text + ' [SEP]'\n",
    "\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    segments_id = [1] * len(tokenized_text)\n",
    "\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "    segments_tensor = torch.tensor([segments_id])\n",
    "\n",
    "    # print(tokens_tensor.size())\n",
    "    # print(segments_tensor.size())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(tokens_tensor, segments_tensor)\n",
    "        # get hidden state from all layers\n",
    "        hidden_states = output[2]\n",
    "\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "    token_vecs_sum = []\n",
    "\n",
    "# `token_embeddings` is a [190 x 13 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Sum the vectors from the last four layers.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "\n",
    "    print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))\n",
    "\n",
    "    return tokenized_text, token_vecs_sum"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "%% script false --no-raise-error\n",
    "def get_term_representation(encoded_text, tokenized_text, term, tokenizer):\n",
    "    tokenized_term = tokenizer.tokenize(term)\n",
    "\n",
    "    term_representations = []\n",
    "\n",
    "    for i in range(len(tokenized_text)):\n",
    "        if tokenized_text[i:i+len(tokenized_term)] == tokenized_term:\n",
    "            term_token_tensors = torch.stack(encoded_text[i:i+len(tokenized_term)])\n",
    "            term_vector = torch.mean(term_token_tensors, dim=0)\n",
    "\n",
    "            term_representations.append(term_vector)\n",
    "\n",
    "    print('Term appearances: %d' % len(term_representations))\n",
    "\n",
    "    return term_representations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 192 x 768\n",
      "Shape is: 158 x 768\n",
      "Term appearances: 0\n",
      "Term appearances: 2\n",
      "Total Appearances: 2\n",
      "Term appearances: 0\n",
      "Term appearances: 2\n",
      "Total Appearances: 2\n",
      "Term appearances: 1\n",
      "Term appearances: 0\n",
      "Total Appearances: 1\n",
      "Term appearances: 1\n",
      "Term appearances: 0\n",
      "Total Appearances: 1\n"
     ]
    }
   ],
   "source": [
    "%% script false --no-raise-error\n",
    "import pandas as pd\n",
    "\n",
    "corpus_tokenized = []\n",
    "corpus_token_vecs = []\n",
    "\n",
    "term_vector_df = pd.DataFrame(columns=kw_list, index=[1975])\n",
    "\n",
    "for text in text_list:\n",
    "    tokenized_text, token_vecs = encode_text(text, model=model, tokenizer=tokenizer)\n",
    "    corpus_tokenized.append(tokenized_text)\n",
    "    corpus_token_vecs.append(token_vecs)\n",
    "\n",
    "for term in kw_list:\n",
    "    term_representations = []\n",
    "    for i in range(len(text_list)):\n",
    "        term_representations.extend(get_term_representation(encoded_text=corpus_token_vecs[i],\n",
    "                                                           tokenized_text=corpus_tokenized[i],\n",
    "                                                           term=term, tokenizer=tokenizer))\n",
    "\n",
    "    print('Total Appearances: %d' % len(term_representations))\n",
    "\n",
    "    term_tensors = torch.stack(term_representations)\n",
    "    term_final_vector = torch.mean(term_tensors, dim=0)\n",
    "\n",
    "    term_vector_df[term] = term_vector_df[term].astype(object)\n",
    "    term_vector_df.at[1975, term] = term_final_vector"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Term Context Evolution Experiment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# with open('term_representation_1985_2005.pickle', 'rb') as f:\n",
    "with open('term_representation_scibert_1985_2005.pickle', 'rb') as f:\n",
    "    term_vector_df = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [],
   "source": [
    "kw_list = term_vector_df.columns.tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [],
   "source": [
    "term_vector_df.dropna(axis=0, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import linregress\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "time_past_buffer = 10\n",
    "time_start = 1995\n",
    "time_end = 2004"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neural network\n",
      "Current Time: 1995\n",
      "Similarity Value:  [0.         0.         0.8248844  0.9451669  0.9292238  0.9600707\n",
      " 0.96855664 0.9357511  0.9509116  0.93303317]\n",
      "Slope Value:  5.607118856039813\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1996\n",
      "Similarity Value:  [0.         0.8161502  0.9416206  0.9377596  0.9613466  0.96860385\n",
      " 0.93977106 0.9676707  0.9352894  0.97609425]\n",
      "Slope Value:  6.018899242245281\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1997\n",
      "Similarity Value:  [0.8009435  0.9591466  0.9465803  0.97618496 0.9765601  0.94945705\n",
      " 0.95451987 0.91591907 0.97428167 0.9784683 ]\n",
      "Slope Value:  28.23357224926642\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1998\n",
      "Similarity Value:  [0.9534458  0.94429207 0.9719892  0.97256875 0.94729185 0.9588229\n",
      " 0.9262146  0.97618145 0.98231214 0.98365885]\n",
      "Slope Value:  67.69470743015243\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1999\n",
      "Similarity Value:  [0.93973243 0.9693913  0.9701036  0.9386368  0.95983255 0.9198681\n",
      " 0.9749342  0.98595285 0.9829153  0.98550534]\n",
      "Slope Value:  70.13633163385235\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2000\n",
      "Similarity Value:  [0.9532821  0.95975417 0.93044436 0.95183253 0.9360796  0.97593194\n",
      " 0.9831667  0.96963406 0.9730418  0.9783054 ]\n",
      "Slope Value:  110.47270213138805\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2001\n",
      "Similarity Value:  [0.96102494 0.93242884 0.9607     0.9453541  0.9787191  0.9835228\n",
      " 0.9769891  0.98093534 0.98331296 0.9843576 ]\n",
      "Slope Value:  128.67111608544636\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2002\n",
      "Similarity Value:  [0.94415253 0.95714486 0.94593906 0.97554034 0.9770208  0.97699094\n",
      " 0.9785999  0.9772962  0.97835964 0.98122346]\n",
      "Slope Value:  174.38294025718247\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2003\n",
      "Similarity Value:  [0.9591984  0.9478303  0.97117364 0.979241   0.97613686 0.98322296\n",
      " 0.97997546 0.97542536 0.98803777 0.9794915 ]\n",
      "Slope Value:  191.13172447918762\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2004\n",
      "Similarity Value:  [0.9422362  0.97578555 0.9835478  0.98009807 0.98245317 0.9804754\n",
      " 0.97871023 0.988417   0.9809798  0.9842613 ]\n",
      "Slope Value:  144.1411367780054\n",
      "---------------------------\n",
      "\n",
      "face detection\n",
      "Current Time: 1995\n",
      "Similarity Value:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Slope Value:  nan\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1996\n",
      "Similarity Value:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Slope Value:  nan\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1997\n",
      "Similarity Value:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.99999976]\n",
      "Slope Value:  5.000001192093179\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1998\n",
      "Similarity Value:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.99999976 0.99999976]\n",
      "Slope Value:  5.000001192093181\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1999\n",
      "Similarity Value:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.99999976 0.99999976 0.99999976]\n",
      "Slope Value:  5.00000119209318\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2000\n",
      "Similarity Value:  [0.         0.         0.         0.         0.         0.\n",
      " 0.82801586 0.82801586 0.82801586 0.82801586]\n",
      "Slope Value:  6.038531648992092\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2001\n",
      "Similarity Value:  [0.         0.         0.         0.         0.         0.82801586\n",
      " 0.82801586 0.82801586 0.82801586 1.0000002 ]\n",
      "Slope Value:  5.907498371943496\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2002\n",
      "Similarity Value:  [0.         0.         0.         0.         0.81016934 0.81016934\n",
      " 0.81016934 0.81016934 0.8281758  0.8281758 ]\n",
      "Slope Value:  6.169547210732883\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2003\n",
      "Similarity Value:  [0.         0.         0.         0.81016934 0.81016934 0.81016934\n",
      " 0.81016934 0.8281758  0.8281758  0.9999998 ]\n",
      "Slope Value:  6.230655129761892\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2004\n",
      "Similarity Value:  [0.         0.         0.81016934 0.81016934 0.81016934 0.81016934\n",
      " 0.8281758  0.8281758  0.9999998  0.9999998 ]\n",
      "Slope Value:  6.507877629926944\n",
      "---------------------------\n",
      "\n",
      "transfer learning\n",
      "Current Time: 1995\n",
      "Similarity Value:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Slope Value:  nan\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1996\n",
      "Similarity Value:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Slope Value:  nan\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1997\n",
      "Similarity Value:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Slope Value:  nan\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1998\n",
      "Similarity Value:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Slope Value:  nan\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1999\n",
      "Similarity Value:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Slope Value:  nan\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2000\n",
      "Similarity Value:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Slope Value:  nan\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2001\n",
      "Similarity Value:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Slope Value:  nan\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2002\n",
      "Similarity Value:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Slope Value:  nan\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2003\n",
      "Similarity Value:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Slope Value:  nan\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2004\n",
      "Similarity Value:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Slope Value:  nan\n",
      "---------------------------\n",
      "\n",
      "artificial intelligence\n",
      "Current Time: 1995\n",
      "Similarity Value:  [0.87312484 0.869832   0.85089105 0.8499392  0.81907624 0.83353543\n",
      " 0.8921617  0.8528771  0.8273193  0.9139589 ]\n",
      "Slope Value:  14.051423835898351\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1996\n",
      "Similarity Value:  [0.6200719  0.7474165  0.706024   0.64042014 0.69802916 0.6857445\n",
      " 0.6335827  0.64660156 0.69237447 0.77471757]\n",
      "Slope Value:  14.75347963927909\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1997\n",
      "Similarity Value:  [0.8422606  0.8492066  0.846797   0.8705783  0.90682447 0.87431276\n",
      " 0.852395   0.9465871  0.92660546 0.76094884]\n",
      "Slope Value:  4.559342178559061\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1998\n",
      "Similarity Value:  [0.86622745 0.83717346 0.8787625  0.9218111  0.8881291  0.83990866\n",
      " 0.9438704  0.9092401  0.72602034 0.9382299 ]\n",
      "Slope Value:  0.5412434945805027\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1999\n",
      "Similarity Value:  [0.85181606 0.7972578  0.83760786 0.843838   0.8148513  0.8961494\n",
      " 0.89631563 0.76757205 0.906628   0.8822701 ]\n",
      "Slope Value:  23.76891957473666\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2000\n",
      "Similarity Value:  [0.81396425 0.8722737  0.8623452  0.79693556 0.9112091  0.9037782\n",
      " 0.6800406  0.90482455 0.8743831  0.9054953 ]\n",
      "Slope Value:  7.3855727804035185\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2001\n",
      "Similarity Value:  [0.9061848  0.85793567 0.86946785 0.92678416 0.91887486 0.75435853\n",
      " 0.92893666 0.9259076  0.9083031  0.8943114 ]\n",
      "Slope Value:  7.266780446144159\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2002\n",
      "Similarity Value:  [0.8758242  0.8549261  0.94525623 0.9076861  0.74517983 0.95124245\n",
      " 0.93829924 0.92500377 0.916439   0.9382427 ]\n",
      "Slope Value:  16.91663668060698\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2003\n",
      "Similarity Value:  [0.87748736 0.92895067 0.9136885  0.73366904 0.9090036  0.9264408\n",
      " 0.89468026 0.87219524 0.9472028  0.9227785 ]\n",
      "Slope Value:  12.558703918888325\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2004\n",
      "Similarity Value:  [0.9378312  0.92058957 0.75127816 0.9443532  0.9351899  0.92278695\n",
      " 0.92214346 0.9486563  0.9469446  0.9213196 ]\n",
      "Slope Value:  15.255417095764653\n",
      "---------------------------\n",
      "\n",
      "object recognition\n",
      "Current Time: 1995\n",
      "Similarity Value:  [0.74480724 0.74480724 0.77858096 0.77858096 0.77858096 0.77858096\n",
      " 0.8215524  0.8237144  0.74826944 0.76255137]\n",
      "Slope Value:  37.01556642482853\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1996\n",
      "Similarity Value:  [0.         0.8830732  0.8830732  0.8830732  0.8830732  0.90721446\n",
      " 0.9182047  0.86935806 0.8559497  0.8097835 ]\n",
      "Slope Value:  5.113972810081702\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1997\n",
      "Similarity Value:  [0.8661157  0.8661157  0.8661157  0.8661157  0.8838531  0.89880574\n",
      " 0.8560331  0.84273374 0.7579787  0.95036745]\n",
      "Slope Value:  -3.143746514293341\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1998\n",
      "Similarity Value:  [0.         0.         0.         0.8982593  0.9096017  0.87305886\n",
      " 0.8557301  0.81118536 0.935042   0.92629075]\n",
      "Slope Value:  5.642108537655913\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1999\n",
      "Similarity Value:  [0.         0.         0.92120296 0.92891586 0.84587735 0.86472166\n",
      " 0.844714   0.9380429  0.9081557  0.94405425]\n",
      "Slope Value:  5.62761982896755\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2000\n",
      "Similarity Value:  [0.         0.9128957  0.91373146 0.8665081  0.8700005  0.8150103\n",
      " 0.9604584  0.9465126  0.9536624  0.949032  ]\n",
      "Slope Value:  6.026528867178917\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2001\n",
      "Similarity Value:  [0.8769444  0.90689206 0.8715638  0.87209374 0.7863397  0.9282635\n",
      " 0.9282936  0.9462817  0.9231604  0.9380226 ]\n",
      "Slope Value:  32.45966158587827\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2002\n",
      "Similarity Value:  [0.91596043 0.85488737 0.8630926  0.8702111  0.944065   0.91488796\n",
      " 0.9400437  0.96042526 0.9485053  0.92728364]\n",
      "Slope Value:  54.30776395086142\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2003\n",
      "Similarity Value:  [0.84931314 0.8300668  0.8046266  0.9339025  0.9109861  0.9293177\n",
      " 0.9331762  0.93911874 0.90110636 0.9341825 ]\n",
      "Slope Value:  43.58049741626145\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2004\n",
      "Similarity Value:  [0.84633356 0.8030353  0.9499158  0.9406547  0.9474257  0.942859\n",
      " 0.94348323 0.93434703 0.9422135  0.9317854 ]\n",
      "Slope Value:  36.19477371698091\n",
      "---------------------------\n",
      "\n",
      "machine learning\n",
      "Current Time: 1995\n",
      "Similarity Value:  [0.         0.8325424  0.8325424  0.8325424  0.8325424  0.80849063\n",
      " 0.80849063 0.85131276 0.8829423  0.8657132 ]\n",
      "Slope Value:  6.383764059548547\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1996\n",
      "Similarity Value:  [0.7855096  0.7855096  0.7855096  0.7855096  0.8020082  0.8020082\n",
      " 0.8363202  0.8785944  0.83283865 0.89476323]\n",
      "Slope Value:  65.07974015766244\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1997\n",
      "Similarity Value:  [0.         0.         0.         0.79157287 0.79157287 0.85215986\n",
      " 0.89757335 0.8696935  0.91289425 0.93010384]\n",
      "Slope Value:  6.147972516866071\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1998\n",
      "Similarity Value:  [0.         0.         0.79157287 0.79157287 0.85215986 0.89757335\n",
      " 0.8696935  0.91289425 0.93010384 1.0000004 ]\n",
      "Slope Value:  6.426111309971932\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1999\n",
      "Similarity Value:  [0.         0.7967061  0.7967061  0.85557616 0.8753703  0.8553365\n",
      " 0.922083   0.8926131  0.92603165 0.92603165]\n",
      "Slope Value:  7.022813668171984\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2000\n",
      "Similarity Value:  [0.84346586 0.84346586 0.8816801  0.909407   0.87566876 0.93891346\n",
      " 0.9493828  0.94570607 0.94570607 0.9318049 ]\n",
      "Slope Value:  62.44766612345769\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2001\n",
      "Similarity Value:  [0.        0.8882507 0.8910371 0.8725749 0.9381603 0.9286913 0.9464255\n",
      " 0.9464255 0.9389961 0.9627298]\n",
      "Slope Value:  6.127921221858354\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2002\n",
      "Similarity Value:  [0.8772285  0.9146277  0.8887558  0.91445434 0.9279789  0.9491199\n",
      " 0.9491199  0.93660027 0.9552393  0.9503695 ]\n",
      "Slope Value:  98.09646963653006\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2003\n",
      "Similarity Value:  [0.9128872  0.8880049  0.9300302  0.9322713  0.94987166 0.94987166\n",
      " 0.9447052  0.9715367  0.96210563 0.9685622 ]\n",
      "Slope Value:  103.58749113500267\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2004\n",
      "Similarity Value:  [0.8855107  0.93777746 0.93707687 0.9418581  0.9418581  0.940026\n",
      " 0.9711685  0.96068335 0.9608164  0.9737202 ]\n",
      "Slope Value:  101.61816409696061\n",
      "---------------------------\n",
      "\n",
      "energy consumption\n",
      "Current Time: 1995\n",
      "Similarity Value:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Slope Value:  nan\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1996\n",
      "Similarity Value:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Slope Value:  nan\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1997\n",
      "Similarity Value:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Slope Value:  nan\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1998\n",
      "Similarity Value:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Slope Value:  nan\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1999\n",
      "Similarity Value:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.8505694]\n",
      "Slope Value:  5.8784149079475965\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2000\n",
      "Similarity Value:  [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.8505694 1.0000001]\n",
      "Slope Value:  5.414157255875078\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2001\n",
      "Similarity Value:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.91055566 0.8961854  0.8961854 ]\n",
      "Slope Value:  5.540663255761136\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2002\n",
      "Similarity Value:  [0.         0.         0.         0.         0.         0.\n",
      " 0.83988595 0.84540606 0.84540606 0.90337056]\n",
      "Slope Value:  5.868861608750821\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2003\n",
      "Similarity Value:  [0.         0.         0.         0.         0.         0.9164367\n",
      " 0.9105146  0.9105146  0.96431637 0.9222169 ]\n",
      "Slope Value:  5.43194647995906\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2004\n",
      "Similarity Value:  [0.         0.         0.         0.         0.9004117  0.8992379\n",
      " 0.8992379  0.9439671  0.90144664 0.95551527]\n",
      "Slope Value:  5.526573003909004\n",
      "---------------------------\n",
      "\n",
      "genetic algorithms\n",
      "Current Time: 1995\n",
      "Similarity Value:  [0.77640605 0.77640605 0.77640605 0.77640605 0.77640605 0.93272275\n",
      " 0.9179101  0.948501   0.91442347 0.95024633]\n",
      "Slope Value:  31.91517527080066\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1996\n",
      "Similarity Value:  [0.         0.         0.         0.         0.932294   0.9051305\n",
      " 0.9456185  0.9089236  0.9439909  0.95514965]\n",
      "Slope Value:  5.406748379136721\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1997\n",
      "Similarity Value:  [0.         0.         0.         0.95772123 0.9510778  0.96798646\n",
      " 0.9366113  0.9715146  0.9567606  0.9523473 ]\n",
      "Slope Value:  5.2257291363695\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1998\n",
      "Similarity Value:  [0.         0.         0.91547716 0.9012507  0.9101542  0.8762934\n",
      " 0.9300001  0.92752516 0.91964173 0.92736304]\n",
      "Slope Value:  5.569524162845753\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1999\n",
      "Similarity Value:  [0.         0.95571285 0.92117876 0.9622814  0.9311267  0.9505527\n",
      " 0.9536191  0.96408314 0.9671077  0.9147681 ]\n",
      "Slope Value:  5.261859165502064\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2000\n",
      "Similarity Value:  [0.9209044  0.9100918  0.9361326  0.88519806 0.95206785 0.93436044\n",
      " 0.9160679  0.958186   0.9089498  0.93161225]\n",
      "Slope Value:  32.192489874250676\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2001\n",
      "Similarity Value:  [0.90576994 0.9159024  0.9129175  0.9258612  0.9415077  0.9122951\n",
      " 0.9351558  0.9139546  0.9199964  0.9265431 ]\n",
      "Slope Value:  97.544113351335\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2002\n",
      "Similarity Value:  [0.94608843 0.91295433 0.96143925 0.9647776  0.9602165  0.95847106\n",
      " 0.9218527  0.9574502  0.9413816  0.94620824]\n",
      "Slope Value:  8.97169781501444\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2003\n",
      "Similarity Value:  [0.91791666 0.94335085 0.93993866 0.9624029  0.95603865 0.90330875\n",
      " 0.9748961  0.92092586 0.9191076  0.956201  ]\n",
      "Slope Value:  6.611160910775936\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2004\n",
      "Similarity Value:  [0.9587189  0.9612074  0.944199   0.9690974  0.93144083 0.95948464\n",
      " 0.9643426  0.94929147 0.9578264  0.94979054]\n",
      "Slope Value:  -29.55896509451372\n",
      "---------------------------\n",
      "\n",
      "virtual machines\n",
      "Current Time: 1995\n",
      "Similarity Value:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Slope Value:  nan\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1996\n",
      "Similarity Value:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Slope Value:  nan\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1997\n",
      "Similarity Value:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Slope Value:  nan\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1998\n",
      "Similarity Value:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Slope Value:  nan\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1999\n",
      "Similarity Value:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Slope Value:  nan\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2000\n",
      "Similarity Value:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Slope Value:  nan\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2001\n",
      "Similarity Value:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.99999964]\n",
      "Slope Value:  5.000001788139983\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2002\n",
      "Similarity Value:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.99999964 0.99999964]\n",
      "Slope Value:  5.000001788139983\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2003\n",
      "Similarity Value:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.99999964 0.99999964 0.99999964]\n",
      "Slope Value:  5.000001788139983\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2004\n",
      "Similarity Value:  [0.         0.         0.         0.         0.         0.\n",
      " 0.99999964 0.99999964 0.99999964 0.99999964]\n",
      "Slope Value:  5.000001788139983\n",
      "---------------------------\n",
      "\n",
      "information extraction\n",
      "Current Time: 1995\n",
      "Similarity Value:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Slope Value:  nan\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1996\n",
      "Similarity Value:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Slope Value:  nan\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1997\n",
      "Similarity Value:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Slope Value:  nan\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1998\n",
      "Similarity Value:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Slope Value:  nan\n",
      "---------------------------\n",
      "\n",
      "Current Time: 1999\n",
      "Similarity Value:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.82556885]\n",
      "Slope Value:  6.056430025037638\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2000\n",
      "Similarity Value:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.84405136 0.8620562 ]\n",
      "Slope Value:  5.868210159297917\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2001\n",
      "Similarity Value:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.84405136 0.8620562  1.0000002 ]\n",
      "Slope Value:  5.58668703950292\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2002\n",
      "Similarity Value:  [0.         0.         0.         0.         0.         0.\n",
      " 0.90314555 0.8759329  0.8743632  0.8743632 ]\n",
      "Slope Value:  5.6438848967393165\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2003\n",
      "Similarity Value:  [0.         0.         0.         0.         0.         0.85794973\n",
      " 0.73045945 0.78867537 0.78867537 0.7705294 ]\n",
      "Slope Value:  6.241620154715509\n",
      "---------------------------\n",
      "\n",
      "Current Time: 2004\n",
      "Similarity Value:  [0.         0.         0.         0.         0.91384053 0.8194927\n",
      " 0.82843053 0.82843053 0.88301736 0.85673034]\n",
      "Slope Value:  5.7980726545765515\n",
      "---------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context_evo_df = pd.DataFrame(columns=kw_list, index=range(time_start, time_end+1))\n",
    "\n",
    "for kw in kw_list:\n",
    "    print(kw)\n",
    "    for current_time in range(time_start, time_end+1):\n",
    "        print(f'Current Time: {current_time}')\n",
    "        period_rep_df = term_vector_df.loc[current_time-time_past_buffer:current_time, [kw]]\n",
    "        tensor_list = period_rep_df[kw].tolist()\n",
    "        tensor_size = tensor_list[0].shape[0]\n",
    "        zero_tensor = torch.tensor([0] * tensor_size, dtype=torch.float)\n",
    "\n",
    "        for i in range(1, len(tensor_list)):\n",
    "            if torch.equal(tensor_list[i], zero_tensor):\n",
    "                tensor_list[i] = tensor_list[i-1]\n",
    "\n",
    "        vector_np_list = [tf.numpy() for tf in tensor_list]\n",
    "\n",
    "        # period_distance = pairwise_distances(vector_np_list, metric='cosine')\n",
    "        # print('Distance Value: ', period_distance[time_past_buffer][:time_past_buffer])\n",
    "        # slope_value = linregress(period_distance[time_past_buffer][:time_past_buffer], range(current_time-time_past_buffer, current_time)).slope\n",
    "\n",
    "        period_similarity = cosine_similarity(vector_np_list)\n",
    "        print('Similarity Value: ', period_similarity[time_past_buffer][:time_past_buffer])\n",
    "        slope_value = linregress(period_similarity[time_past_buffer][:time_past_buffer], range(current_time-time_past_buffer, current_time)).slope\n",
    "\n",
    "        print('Slope Value: ', slope_value)\n",
    "        print('---------------------------\\n')\n",
    "        context_evo_df.at[current_time, kw] = slope_value\n",
    "\n",
    "with open('context_evo.pickle', 'wb') as f:\n",
    "    pickle.dump(context_evo_df, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_regression(data_series, time_span_onward = 3):\n",
    "    time_stamp = data_series.index.tolist()\n",
    "    array_data = data_series.tolist()\n",
    "    slope_list = []\n",
    "    for i in range(len(array_data) - time_span_onward):\n",
    "        period_mean = sum(time_stamp[i:i+time_span_onward])/time_span_onward\n",
    "        data_mean = sum(array_data[i:i+time_span_onward])/time_span_onward\n",
    "        dividend = 0\n",
    "        divisor = 0\n",
    "        for j, k in zip(time_stamp[i:i+3], array_data[i:i+3]):\n",
    "            dividend += (j - period_mean) * (k - data_mean)\n",
    "            divisor += (j - period_mean) ** 2\n",
    "        slope_list.append(dividend/divisor)\n",
    "    return pd.Series(slope_list, index=time_stamp[:-time_span_onward])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_path = '/Users/khoanguyen/Workspace/dataset/trendnert/trendnert_partial.gz'\n",
    "trendnert_data = pd.read_json(data_path, lines=True, compression='gzip')\n",
    "\n",
    "trendnert_data_not_null = trendnert_data.loc[trendnert_data['label'].notnull()]\n",
    "\n",
    "label_list = trendnert_data['label'].dropna().unique().tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "topic_frequency_df = pd.DataFrame(columns=label_list)\n",
    "\n",
    "for label in label_list:\n",
    "    label_specific_df = trendnert_data_not_null.loc[trendnert_data_not_null['label'].str.contains(label, regex=False)]\n",
    "    topic_frequency_df[label] = label_specific_df.loc[(label_specific_df['year'] >= 1995) &\n",
    "                                                      (label_specific_df['year'] < 2010)]['year'].value_counts().sort_inde()\n",
    "\n",
    "freq_regression_df = pd.DataFrame(columns=label_list)\n",
    "for label in label_list:\n",
    "    regression = calculate_regression(topic_frequency_df[label].fillna(0))\n",
    "    freq_regression_df[label] = regression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def evaluate_results(topic_kw_map, freq_regression_df, context_evo_df):\n",
    "    for kw in context_evo_df.columns.tolist():\n",
    "        print('Keyword: ', kw)\n",
    "        mapped_label = []\n",
    "        for topic in topic_kw_map:\n",
    "            if kw in topic['keywords']:\n",
    "                mapped_label.append(topic['label'])\n",
    "        prediction = context_evo_df[kw] > 0\n",
    "        for label in mapped_label:\n",
    "            print('Corresponding Label: ', label)\n",
    "            label_trend = freq_regression_df[label] > 0\n",
    "            evaluation = metrics.classification_report(label_trend.tolist()[:-2], prediction.tolist())\n",
    "            print(evaluation)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/Users/khoanguyen/Workspace/git/trendnert_experiment/kw_label_map_gensim.json', 'r+') as f:\n",
    "    topic_kw_map = json.load(f)\n",
    "\n",
    "evaluate_results(topic_kw_map=topic_kw_map, freq_regression_df=freq_regression_df, context_evo_df=context_evo_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}